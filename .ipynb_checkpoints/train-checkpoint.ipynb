{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### image resize\n",
    "from PIL import Image\n",
    "import os\n",
    "size = 256\n",
    "\n",
    "for i in range(6):  # 6 categories\n",
    "    for info in os.listdir(r'./training_images_original/class'+str(i)):\n",
    "        image = Image.open('./training_images_original/class'+str(i)+'/'+info)\n",
    "        image = image.resize((size, size), Image.BILINEAR)\n",
    "        image.save('./training_images/'+info)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "from PIL import Image\n",
    "import os\n",
    "size = 256\n",
    "\n",
    "for info in os.listdir(r'./training7'):\n",
    "    image = Image.open('./training7/'+info)\n",
    "    image = image.resize((size, size), Image.BILINEAR)\n",
    "    image.save('./temp2/'+info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### image augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from mxnet import image\n",
    "from PIL import Image\n",
    "import mxnet as mx\n",
    "from mxnet import gluon, init, nd, autograd\n",
    "from mxnet.gluon import data as gdata, nn, loss as gloss, utils as gutils\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def show_images(imgs, num_rows, num_cols, info, scale=2):\n",
    "    figsize = (num_cols * scale, num_rows * scale)\n",
    "    for i in range(num_rows):\n",
    "        for j in range(num_cols):\n",
    "            plt.imshow(imgs[i * num_cols + j].asnumpy())\n",
    "            plt.axes().get_xaxis().set_visible(False)\n",
    "            plt.axes().get_yaxis().set_visible(False)\n",
    "            plt.savefig('./temp2/'+info[0]+'_IMG_'+str(cnt)+'.jpg',  bbox_inches='tight',pad_inches=0.0)\n",
    "            #plt.show()\n",
    "            global cnt\n",
    "            cnt += 1\n",
    "\n",
    "cnt = 1400\n",
    "#Most image augmentation methods have a certain degree of randomness. This function runs the image augmentation method aug multiple times on the input image img and shows all results.\n",
    "def apply(img, aug, info, num_rows=2, num_cols=2, scale=3):\n",
    "    Y = [aug(img) for _ in range(num_rows * num_cols)] # a list of augmented images\n",
    "    show_images(Y, num_rows, num_cols, info, scale)\n",
    "color_aug = gdata.vision.transforms.RandomColorJitter(\n",
    "    brightness=0.5, contrast=0.5, saturation=0.5, hue=0.5) #明亮度 +-50%\n",
    "#shape_aug = gdata.vision.transforms.RandomResizedCrop(  \n",
    "#(250, 250), scale=(0.1, 1), ratio=(0.5, 2))  \n",
    "\n",
    "augs = gdata.vision.transforms.Compose([\n",
    "        gdata.vision.transforms.RandomFlipLeftRight(), color_aug]) #三個一起\n",
    "augs2= gdata.vision.transforms.Compose([\n",
    "        gdata.vision.transforms.RandomFlipTopBottom(), color_aug]) #三個一起\n",
    "\n",
    "for i, info in enumerate(os.listdir(r'./temp')):\n",
    "    img = image.imread('./temp/'+info)\n",
    "    if i % 2 ==1:\n",
    "        apply(img, augs, info)\n",
    "    else:\n",
    "        apply(img, augs2, info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3878, 256, 256, 3) (3878,)\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import mxnet as mx\n",
    "from mxnet import gluon, init, nd, autograd\n",
    "from mxnet.gluon import data as gdata, nn, loss as gloss, utils as gutils\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "y_list = []\n",
    "X_list = []\n",
    "#使用PIL 來讀圖片，並存成nd.array\n",
    "for i, info in enumerate(os.listdir(r'./training7')):\n",
    "    img = Image.open('./training7/'+info)\n",
    "    X_list.append(np.array(img))\n",
    "    y_list.append(int(info[0]))\n",
    "    \n",
    "X_array = np.array(X_list)\n",
    "y_array = np.array(y_list)\n",
    "\n",
    "print(X_array.shape, y_array.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### generate dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split \n",
    "X_train, X_test, y_train, y_test = train_test_split(X_array,y_array, test_size=0.1) \n",
    "\n",
    "train_dataset = mx.gluon.data.ArrayDataset(nd.array(X_train), nd.array(y_train))\n",
    "test_dataset = mx.gluon.data.ArrayDataset(nd.array(X_test), nd.array(y_test))\n",
    "#這裡的X的形狀為(N, 3, 256, 256)，N是圖片數量\n",
    "#y是圖片的label(N,1)\n",
    "#data_loader = mx.gluon.data.DataLoader(dataset, batch_size = 5, num_workers = 1)\n",
    "#for X, y in data_loader:\n",
    "#    print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''DenseNet '''\n",
    "def conv_block(num_channels): \n",
    "    blk = nn.Sequential() \n",
    "    blk.add(nn.BatchNorm(), \n",
    "            nn.Activation('relu'),\n",
    "            nn.Conv2D(num_channels, kernel_size=3, padding=1))\n",
    "    return blk\n",
    "class DenseBlock(nn.Block): \n",
    "    def __init__(self, num_convs, num_channels, **kwargs):\n",
    "        super(DenseBlock, self).__init__(**kwargs) \n",
    "        self.net = nn.Sequential() \n",
    "        for _ in range(num_convs): \n",
    "            self.net.add(conv_block(num_channels))\n",
    "            \n",
    "    def forward(self, X):\n",
    "        for blk in self.net:\n",
    "            Y = blk(X)\n",
    "            # Concatenate the input and output of each block on the channel \n",
    "            # dimension\n",
    "            X = nd.concat(X, Y, dim=1) \n",
    "\n",
    "        return X\n",
    "def transition_block(num_channels):\n",
    "    blk = nn.Sequential() \n",
    "    blk.add(nn.BatchNorm(), \n",
    "            nn.Activation('relu'),\n",
    "            nn.Conv2D(num_channels, kernel_size=1), \n",
    "            nn.AvgPool2D(pool_size=2, strides=2))\n",
    "    return blk\n",
    "net = nn.Sequential()\n",
    "net.add(nn.Conv2D(64, kernel_size=7, strides=2, padding=3), \n",
    "        nn.BatchNorm(),\n",
    "        nn.Activation('relu'), \n",
    "        nn.MaxPool2D(pool_size=3, strides=2, padding=1))\n",
    "num_channels, growth_rate = 64, 32\n",
    "num_convs_in_dense_blocks = [4, 4, 4, 4]\n",
    "\n",
    "for i, num_convs in enumerate(num_convs_in_dense_blocks):\n",
    "    net.add(DenseBlock(num_convs, growth_rate))\n",
    "    # This is the number of output channels in the previous dense block \n",
    "    num_channels += num_convs * growth_rate\n",
    "    # A transition layer that haves the number of channels is added between\n",
    "    # the dense blocks \n",
    "    if i != len(num_convs_in_dense_blocks) - 1: \n",
    "        num_channels //= 2\n",
    "        net.add(transition_block(num_channels))\n",
    "net.add(nn.BatchNorm(), \n",
    "        nn.Activation('relu'), \n",
    "        nn.GlobalAvgPool2D(),\n",
    "        nn.Dense(6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.Training a Model and Evaluate on a test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training on gpu(0)\n",
      "epoch 1, loss 1.9007, train acc 0.191, test acc 0.188, time 293.8 sec\n",
      "epoch 2, loss 1.8054, train acc 0.197, test acc 0.178, time 209.4 sec\n",
      "epoch 3, loss 1.7925, train acc 0.205, test acc 0.222, time 210.8 sec\n",
      "epoch 4, loss 1.7715, train acc 0.226, test acc 0.227, time 210.0 sec\n",
      "epoch 5, loss 1.7653, train acc 0.234, test acc 0.237, time 209.9 sec\n",
      "epoch 6, loss 1.7479, train acc 0.243, test acc 0.235, time 211.0 sec\n",
      "epoch 7, loss 1.7297, train acc 0.263, test acc 0.263, time 211.3 sec\n",
      "epoch 8, loss 1.7205, train acc 0.274, test acc 0.271, time 212.4 sec\n",
      "epoch 9, loss 1.6778, train acc 0.298, test acc 0.317, time 214.8 sec\n",
      "epoch 10, loss 1.6680, train acc 0.301, test acc 0.304, time 210.0 sec\n",
      "epoch 11, loss 1.6247, train acc 0.331, test acc 0.278, time 209.0 sec\n",
      "epoch 12, loss 1.5775, train acc 0.358, test acc 0.307, time 208.9 sec\n",
      "epoch 13, loss 1.5116, train acc 0.389, test acc 0.332, time 212.9 sec\n",
      "epoch 14, loss 1.4140, train acc 0.429, test acc 0.322, time 208.7 sec\n",
      "epoch 15, loss 1.3310, train acc 0.468, test acc 0.371, time 208.9 sec\n",
      "epoch 16, loss 1.2399, train acc 0.498, test acc 0.446, time 208.8 sec\n",
      "epoch 17, loss 1.1551, train acc 0.540, test acc 0.451, time 208.8 sec\n",
      "epoch 18, loss 1.0689, train acc 0.577, test acc 0.472, time 209.0 sec\n",
      "epoch 19, loss 0.9770, train acc 0.635, test acc 0.500, time 208.8 sec\n",
      "epoch 20, loss 0.8639, train acc 0.672, test acc 0.479, time 208.8 sec\n",
      "epoch 21, loss 0.7830, train acc 0.713, test acc 0.616, time 208.8 sec\n",
      "epoch 22, loss 0.6759, train acc 0.766, test acc 0.678, time 208.9 sec\n",
      "epoch 23, loss 0.5742, train acc 0.791, test acc 0.673, time 210.1 sec\n",
      "epoch 24, loss 0.4715, train acc 0.846, test acc 0.595, time 209.3 sec\n",
      "epoch 25, loss 0.3839, train acc 0.873, test acc 0.678, time 210.0 sec\n",
      "epoch 26, loss 0.3272, train acc 0.895, test acc 0.735, time 211.7 sec\n",
      "epoch 27, loss 0.2627, train acc 0.919, test acc 0.781, time 251.7 sec\n",
      "epoch 28, loss 0.2103, train acc 0.936, test acc 0.737, time 254.2 sec\n",
      "epoch 29, loss 0.1589, train acc 0.954, test acc 0.737, time 225.2 sec\n",
      "epoch 30, loss 0.1360, train acc 0.963, test acc 0.722, time 240.4 sec\n",
      "epoch 31, loss 0.1119, train acc 0.967, test acc 0.765, time 213.4 sec\n",
      "epoch 32, loss 0.0817, train acc 0.980, test acc 0.814, time 274.9 sec\n",
      "epoch 33, loss 0.0689, train acc 0.985, test acc 0.838, time 299.3 sec\n",
      "epoch 34, loss 0.0654, train acc 0.984, test acc 0.830, time 215.0 sec\n",
      "epoch 35, loss 0.0490, train acc 0.989, test acc 0.830, time 216.3 sec\n",
      "epoch 36, loss 0.0433, train acc 0.991, test acc 0.809, time 210.4 sec\n",
      "epoch 37, loss 0.0198, train acc 0.997, test acc 0.853, time 218.6 sec\n",
      "epoch 38, loss 0.0328, train acc 0.993, test acc 0.869, time 209.2 sec\n",
      "epoch 39, loss 0.0294, train acc 0.992, test acc 0.845, time 210.1 sec\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-6393783589c4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     92\u001b[0m \u001b[0mtrainer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgluon\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTrainer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollect_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msgd_optimizer\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m###\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[1;31m#trainer = gluon.Trainer(net.collect_params(), optimizer=adam_optimizer) ###\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 94\u001b[1;33m \u001b[0mtrain_ch5\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_iter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_iter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     95\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[0mnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave_parameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'net.params'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-6393783589c4>\u001b[0m in \u001b[0;36mtrain_ch5\u001b[1;34m(net, train_iter, test_iter, batch_size, trainer, ctx, num_epochs)\u001b[0m\n\u001b[0;32m     30\u001b[0m             \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'float32'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m             \u001b[0mtrain_l_sum\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masscalar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m             \u001b[0mtrain_acc_sum\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0my_hat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masscalar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m             \u001b[0mn\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m         \u001b[0mtest_acc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mevaluate_accuracy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_iter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnet\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\aduser01\\Anaconda3\\envs\\mxnet\\lib\\site-packages\\mxnet\\ndarray\\ndarray.py\u001b[0m in \u001b[0;36masscalar\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   2012\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2013\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"The current array is not a scalar\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2014\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2015\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2016\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\aduser01\\Anaconda3\\envs\\mxnet\\lib\\site-packages\\mxnet\\ndarray\\ndarray.py\u001b[0m in \u001b[0;36masnumpy\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1994\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1995\u001b[0m             \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mctypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata_as\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mctypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mc_void_p\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1996\u001b[1;33m             ctypes.c_size_t(data.size)))\n\u001b[0m\u001b[0;32m   1997\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1998\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import mxnet as mx\n",
    "from mxnet import gluon, init, nd, autograd\n",
    "from mxnet.gluon import data as gdata, nn, loss as gloss, utils as gutils\n",
    "import sys\n",
    "import time\n",
    "\n",
    "def try_gpu():\n",
    "    \"\"\"If GPU is available, return mx.gpu(0); else return mx.cpu().\"\"\"\n",
    "    try:\n",
    "        ctx = mx.gpu()\n",
    "        _ = nd.array([0], ctx=ctx)\n",
    "    except mx.base.MXNetError:\n",
    "        ctx = mx.cpu()\n",
    "    return ctx\n",
    "\n",
    "def train_ch5(net, train_iter, test_iter, batch_size, trainer, ctx,\n",
    "              num_epochs):\n",
    "    \"\"\"Train and evaluate a model with CPU or GPU.\"\"\"\n",
    "    print('training on', ctx)\n",
    "    loss = gloss.SoftmaxCrossEntropyLoss()\n",
    "    for epoch in range(num_epochs):\n",
    "        train_l_sum, train_acc_sum, n, start = 0.0, 0.0, 0, time.time()\n",
    "        for X, y in train_iter:\n",
    "            X, y = X.as_in_context(ctx), y.as_in_context(ctx)\n",
    "            with autograd.record():\n",
    "                y_hat = net(X)            \n",
    "                l = loss(y_hat, y).sum()\n",
    "            l.backward()\n",
    "            trainer.step(batch_size)\n",
    "            y = y.astype('float32')\n",
    "            train_l_sum += l.asscalar()\n",
    "            train_acc_sum += (y_hat.argmax(axis=1) == y).sum().asscalar()\n",
    "            n += y.size\n",
    "        test_acc = evaluate_accuracy(test_iter, net, ctx)\n",
    "        print('epoch %d, loss %.4f, train acc %.3f, test acc %.3f, '\n",
    "              'time %.1f sec'\n",
    "              % (epoch + 1, train_l_sum / n, train_acc_sum / n, test_acc,\n",
    "                 time.time() - start))\n",
    "        \n",
    "def evaluate_accuracy(data_iter, net, ctx=[mx.cpu()]):\n",
    "    \"\"\"Evaluate accuracy of a model on the given data set.\"\"\"\n",
    "    if isinstance(ctx, mx.Context):\n",
    "        ctx = [ctx]\n",
    "    acc_sum, n = nd.array([0]), 0\n",
    "    for batch in data_iter:\n",
    "        features, labels, _ = _get_batch(batch, ctx)\n",
    "        for X, y in zip(features, labels):\n",
    "            y = y.astype('float32')\n",
    "            acc_sum += (net(X).argmax(axis=1) == y).sum().copyto(mx.cpu())\n",
    "            n += y.size\n",
    "        acc_sum.wait_to_read()\n",
    "    return acc_sum.asscalar() / n\n",
    "\n",
    "def _get_batch(batch, ctx):\n",
    "    \"\"\"Return features and labels on ctx.\"\"\"\n",
    "    features, labels = batch\n",
    "    if labels.dtype != features.dtype:\n",
    "        labels = labels.astype(features.dtype)\n",
    "    return (gutils.split_and_load(features, ctx),\n",
    "            gutils.split_and_load(labels, ctx), features.shape[0])\n",
    "\n",
    "\n",
    "#####\n",
    "def load_data(batch_size, resize=None):\n",
    "    transformer = []\n",
    "    if resize:\n",
    "        transformer += [gdata.vision.transforms.Resize(resize)]\n",
    "    transformer += [gdata.vision.transforms.ToTensor()]\n",
    "    transformer += [gdata.vision.transforms.Normalize(0.13, 0.31)]\n",
    "    transformer = gdata.vision.transforms.Compose(transformer)\n",
    "\n",
    "    num_workers = 0 if sys.platform.startswith('win32') else 4\n",
    "    train_iter = gdata.DataLoader(\n",
    "        train_dataset.transform_first(transformer), batch_size, shuffle=True,\n",
    "        num_workers=num_workers)\n",
    "    test_iter = gdata.DataLoader(\n",
    "        test_dataset.transform_first(transformer), batch_size, shuffle=True,\n",
    "        num_workers=num_workers)\n",
    "    return train_iter, test_iter\n",
    "    \n",
    "\n",
    "lr, num_epochs, ctx = 0.05, 70, try_gpu()\n",
    "\n",
    "##########\n",
    "batch_size = 2\n",
    "train_iter, test_iter = load_data(batch_size)\n",
    "\n",
    "sgd_optimizer = mx.optimizer.SGD(learning_rate=lr)\n",
    "adam_optimizer = mx.optimizer.Adam(learning_rate=lr, beta1=0.8, beta2=0.9)\n",
    "\n",
    "net.initialize(force_reinit=True, ctx=ctx, init=init.Xavier())\n",
    "trainer = gluon.Trainer(net.collect_params(), optimizer=sgd_optimizer) ###\n",
    "#trainer = gluon.Trainer(net.collect_params(), optimizer=adam_optimizer) ###\n",
    "train_ch5(net, train_iter, test_iter, batch_size, trainer, ctx, num_epochs)\n",
    "\n",
    "net.save_parameters('net.params')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "net.save_parameters('net.params')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Predict with a pre-trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(300, 256, 256, 3) (300,)\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import mxnet as mx\n",
    "from mxnet import gluon, init, nd, autograd\n",
    "from mxnet.gluon import data as gdata, nn, loss as gloss, utils as gutils\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "y_test_list = []\n",
    "X_test_list = []\n",
    "#使用PIL 來讀圖片，並存成nd.array\n",
    "for i, info in enumerate(os.listdir(r'./validation_images')):\n",
    "    y_test_list.append(int(info[0]))\n",
    "    image = Image.open('./validation_images/'+info)\n",
    "    image2array = np.array(image)   \n",
    "    X_test_list.append(image2array)\n",
    "X_test_array = nd.array(X_test_list)\n",
    "y_test_array = nd.array(y_test_list)\n",
    "\n",
    "print(X_test_array.shape, y_test_array.shape)\n",
    "\n",
    "test_dataset = mx.gluon.data.ArrayDataset(X_test_array, y_test_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''DenseNet '''\n",
    "def conv_block(num_channels): \n",
    "    blk = nn.Sequential() \n",
    "    blk.add(nn.BatchNorm(), \n",
    "            nn.Activation('relu'),\n",
    "            nn.Conv2D(num_channels, kernel_size=3, padding=1))\n",
    "    return blk\n",
    "class DenseBlock(nn.Block): \n",
    "    def __init__(self, num_convs, num_channels, **kwargs):\n",
    "        super(DenseBlock, self).__init__(**kwargs) \n",
    "        self.net = nn.Sequential() \n",
    "        for _ in range(num_convs): \n",
    "            self.net.add(conv_block(num_channels))\n",
    "            \n",
    "    def forward(self, X):\n",
    "        for blk in self.net:\n",
    "            Y = blk(X)\n",
    "            # Concatenate the input and output of each block on the channel \n",
    "            # dimension\n",
    "            X = nd.concat(X, Y, dim=1) \n",
    "\n",
    "        return X\n",
    "def transition_block(num_channels):\n",
    "    blk = nn.Sequential() \n",
    "    blk.add(nn.BatchNorm(), \n",
    "            nn.Activation('relu'),\n",
    "            nn.Conv2D(num_channels, kernel_size=1), \n",
    "            nn.AvgPool2D(pool_size=2, strides=2))\n",
    "    return blk\n",
    "net = nn.Sequential()\n",
    "net.add(nn.Conv2D(64, kernel_size=7, strides=2, padding=3), \n",
    "        nn.BatchNorm(),\n",
    "        nn.Activation('relu'), \n",
    "        nn.MaxPool2D(pool_size=3, strides=2, padding=1))\n",
    "num_channels, growth_rate = 64, 32\n",
    "num_convs_in_dense_blocks = [4, 4, 4, 4]\n",
    "\n",
    "for i, num_convs in enumerate(num_convs_in_dense_blocks):\n",
    "    net.add(DenseBlock(num_convs, growth_rate))\n",
    "    # This is the number of output channels in the previous dense block \n",
    "    num_channels += num_convs * growth_rate\n",
    "    # A transition layer that haves the number of channels is added between\n",
    "    # the dense blocks \n",
    "    if i != len(num_convs_in_dense_blocks) - 1: \n",
    "        num_channels //= 2\n",
    "        net.add(transition_block(num_channels))\n",
    "net.add(nn.BatchNorm(), \n",
    "        nn.Activation('relu'), \n",
    "        nn.GlobalAvgPool2D(),\n",
    "        nn.Dense(6))\n",
    "\n",
    "net.load_parameters('net.params')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.966666666667\n"
     ]
    }
   ],
   "source": [
    "from mxnet import nd\n",
    "from mxnet import gluon\n",
    "from mxnet.gluon.data.vision import datasets, transforms\n",
    "\n",
    "batch_size = 2\n",
    "###the same transformation\n",
    "def try_gpu():\n",
    "    \"\"\"If GPU is available, return mx.gpu(0); else return mx.cpu().\"\"\"\n",
    "    try:\n",
    "        ctx = mx.gpu()\n",
    "        _ = nd.array([0], ctx=ctx)\n",
    "    except mx.base.MXNetError:\n",
    "        ctx = mx.cpu()\n",
    "    return ctx\n",
    "        \n",
    "def evaluate_accuracy(data_iter, net, ctx=[mx.cpu()]):\n",
    "    \"\"\"Evaluate accuracy of a model on the given data set.\"\"\"\n",
    "    if isinstance(ctx, mx.Context):\n",
    "        ctx = [ctx]\n",
    "    acc_sum, n = nd.array([0]), 0\n",
    "    for batch in data_iter:\n",
    "        features, labels, _ = _get_batch(batch, ctx)\n",
    "        for X, y in zip(features, labels):\n",
    "            y = y.astype('float32')\n",
    "            acc_sum += (net(X).argmax(axis=1) == y).sum().copyto(mx.cpu())\n",
    "            n += y.size\n",
    "        acc_sum.wait_to_read()\n",
    "    return acc_sum.asscalar() / n\n",
    "\n",
    "def _get_batch(batch, ctx):\n",
    "    \"\"\"Return features and labels on ctx.\"\"\"\n",
    "    features, labels = batch\n",
    "    if labels.dtype != features.dtype:\n",
    "        labels = labels.astype(features.dtype)\n",
    "    return (gutils.split_and_load(features, ctx),\n",
    "            gutils.split_and_load(labels, ctx), features.shape[0])\n",
    "\n",
    "\n",
    "transformer = []\n",
    "transformer += [gdata.vision.transforms.ToTensor()]\n",
    "transformer += [gdata.vision.transforms.Normalize(0.13, 0.31)]\n",
    "transformer = gdata.vision.transforms.Compose(transformer)\n",
    "\n",
    "test_iter = gdata.DataLoader(\n",
    "        test_dataset.transform_first(transformer), batch_size, shuffle=False)\n",
    "\n",
    "#####\n",
    "\n",
    "ctx = try_gpu()    \n",
    "test_acc = evaluate_accuracy(test_iter, net)\n",
    "print(test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
